import pandas as pd
import requests
import time
import argparse
import os
import base64
import logging
import json
import random

# JIRA credentials
jira_url = "https://dev.jira.kenvue.com"  # Replace with your JIRA Data Center URL
username = "SA-JX2-GITHUB-MIGRAT"  # Replace with your JIRA username
password = '7yk]"Z6uLc/iNZ85'  # Replace with your JIRA password

# Encode the credentials
credentials = f"{username}:{password}"
encoded_credentials = base64.b64encode(credentials.encode()).decode()

# JIRA headers
jira_headers = {
    "Authorization": f"Basic {encoded_credentials}",
    "Content-Type": "application/json"
}

# GitHub credentials
owner = "KENVUE-DEVEX"
github_tokens = [
]

current_token_index = 0

def get_github_headers():
    return {
        "Accept": "application/vnd.github+json",
        "Authorization": f"Bearer {github_tokens[current_token_index]}",
        "X-GitHub-Api-Version": "2022-11-28"
    }

github_headers = get_github_headers()

# Retry logic
def retry_request(func, *args, **kwargs):
    global current_token_index
    global github_headers

    max_retries = 5  # Maximum number of retries
    initial_backoff_time = 90  # Initial backoff time in seconds
    increment = 30  # Increment time in seconds
    backoff_time = initial_backoff_time

    for attempt in range(max_retries):
        response = func(*args, **kwargs)  # Make the original API request
        time.sleep(3)  # Throttle by adding a 3-second delay after each API call
        handle_rate_limit(response)  # Handle rate limits

        if response.status_code in [200, 201, 404]:
            return response  # Return the response if successful
        elif response.status_code == 403:
            # Check remaining rate limit
            if 'X-RateLimit-Remaining' in response.headers:
                remaining = int(response.headers['X-RateLimit-Remaining'])
                if remaining <= 25:
                    logging.warning("Rate limit low. Switching to next token.")
                    # Switch to the next token
                    current_token_index = (current_token_index + 1) % len(github_tokens)
                    logging.info(f"Switched to token index {current_token_index}.")
                    # Update the headers with the new token
                    github_headers = get_github_headers()

            logging.warning(f"Rate limit hit. Retrying in {backoff_time} seconds...")
            time.sleep(backoff_time + random.uniform(0, 1))  # Wait with a buffer

            # After the first retry (attempt index 1), run the random API call
            if attempt >= 1:
                make_random_api_call()

            backoff_time += increment  # Increment the backoff time by 180 seconds
        else:
            try:
                error_message = response.json()
            except ValueError:
                error_message = response.text
            logging.error(f"Request failed: {error_message}")
            if attempt == max_retries - 1:
                return response  # Return the response if maximum retries exceeded

        # Before attempting the fourth retry (attempt index 3), print the number of remaining requests and switch the token
        if attempt == max_retries - 2:
            if 'X-RateLimit-Remaining' in response.headers:
                remaining = int(response.headers['X-RateLimit-Remaining'])
                logging.info(f"Remaining rate limit before fourth retry: {remaining}")
                logging.warning("Switching to next token before fourth retry.")
                # Switch to the next token
                current_token_index = (current_token_index + 1) % len(github_tokens)
                logging.info(f"Switched to token index {current_token_index}.")
                # Update the headers with the new token
                github_headers = get_github_headers()

    logging.error(f"Exceeded maximum retries. Marking as failed.")
    return response  # Return the response if all retries fail

# Function to sanitize repository names
def sanitize_repo_name(repo_name):
    return repo_name.strip().replace(' ', '-')

# Function to sanitize label names
def sanitize_label_name(label_name):
    if isinstance(label_name, str):
        return label_name.strip().replace(' ', '-').replace('_', '-')
    else:
        return None

# Function to create a label in GitHub
def create_label(repo, label_name):
    if label_name:  # Ensure label_name is not None
        url = f"https://api.github.com/repos/{owner}/{repo}/labels"
        payload = {
            "name": label_name,
            "color": "f29513"  # You can customize the color
        }
        response = retry_request(requests.post, url, headers=github_headers, json=payload)
        if response.status_code == 201:
            logging.info(f"Label '{label_name}' created successfully.")
        else:
            logging.error(f"Failed to create label '{label_name}': {response.json()}")

# Function to check if a label exists in GitHub
def label_exists(repo, label_name):
    if label_name:  # Ensure label_name is not None
        url = f"https://api.github.com/repos/{owner}/{repo}/labels/{label_name}"
        logging.info(f"Checking if label exists: {url}")
        response = retry_request(requests.get, url, headers=github_headers)
        if response.status_code == 200:
            logging.info(f"Label '{label_name}' exists in repository '{repo}'.")
            return True
        elif response.status_code == 404:
            logging.info(f"Label '{label_name}' does not exist in repository '{repo}'.")
            return False
        else:
            logging.error(f"Failed to check if label '{label_name}' exists in repository '{repo}': {response.json()}")
            return False
    return False

# Function to create an issue in GitHub
def create_issue(repo, title, body, labels):
    url = f"https://api.github.com/repos/{owner}/{repo}/issues"
    payload = {
        "title": title,
        "body": body,
        "labels": labels
    }
    response = retry_request(requests.post, url, headers=github_headers, json=payload)
    if response.status_code == 201:
        logging.info(f"Issue '{title}' created successfully.")
        return response.json()["number"]
    else:
        try:
            error_message = response.json()
        except ValueError:  # includes simplejson.decoder.JSONDecodeError
            error_message = response.text
        logging.error(f"Failed to create issue '{title}': {error_message}")
        return None

# Function to close an issue in GitHub
def close_issue(repo, issue_number):
    url = f"https://api.github.com/repos/{owner}/{repo}/issues/{issue_number}"
    payload = {
        "state": "closed"
    }
    response = retry_request(requests.patch, url, headers=github_headers, json=payload)
    if response.status_code == 200:
        logging.info(f"Issue #{issue_number} closed successfully.")
    else:
        logging.error(f"Failed to close issue #{issue_number}: {response.json()}")

# Function to add a comment to an issue in GitHub
def add_comment(repo, issue_number, comment):
    url = f"https://api.github.com/repos/{owner}/{repo}/issues/{issue_number}/comments"
    payload = {
        "body": comment
    }
    response = retry_request(requests.post, url, headers=github_headers, json=payload)
    if response.status_code == 201:
        logging.info(f"Comment added to issue #{issue_number}.")
    else:
        logging.error(f"Failed to add comment to issue #{issue_number}: {response.json()}")

# Function to parse comments from the CSV format
def parse_comments(comment_str):
    comments = comment_str.split('|||')
    parsed_comments = []
    for comment in comments:
        parts = comment.split(';')
        if len(parts) >= 3:
            timestamp = parts[0].strip()
            user_id = parts[1].strip()
            comment_text = ';'.join(parts[2:]).strip()
            parsed_comments.append(f"{timestamp} - {user_id}: {comment_text}")
    return parsed_comments

# Function to convert comments to Markdown
def convert_comments_to_markdown(comments):
    markdown_comments = []
    for comment in comments:
        markdown_comments.append(f"> {comment.replace('\n', '\n> ')}")
    return markdown_comments

# Function to handle rate limiting with exponential backoff
def handle_rate_limit(response):
    global current_token_index
    global github_headers

    if response.status_code == 403:
        if 'X-RateLimit-Remaining' in response.headers:
            remaining = int(response.headers['X-RateLimit-Remaining'])
            if remaining == 0:
                logging.warning("Primary rate limit exceeded. Switching to next token.")
                # Switch to the next token
                current_token_index = (current_token_index + 1) % len(github_tokens)
                logging.info(f"Switched to token index {current_token_index}.")
                # Update the headers with the new token
                github_headers = get_github_headers()
        elif 'Retry-After' in response.headers:
            retry_after = int(response.headers['Retry-After'])
            wait_time = max(retry_after, 60)  # Ensure a minimum wait time of 60 seconds
            logging.warning(f"Secondary rate limit exceeded. Sleeping for {wait_time} seconds.")
            time.sleep(wait_time + random.uniform(0, 1))  # Adding a buffer of 1 second
        else:
            error_message = response.json().get('message', '')
            if 'secondary rate limit' in error_message:
                initial_wait_time = 60  # Initial wait time in seconds
                increment = 30  # Increment time in seconds
                backoff_time = initial_wait_time
                while response.status_code == 403:
                    logging.warning(f"Secondary rate limit exceeded. Sleeping for {backoff_time} seconds.")
                    time.sleep(backoff_time + random.uniform(0, 1))
                    backoff_time += increment  # Increase the wait time by 30 seconds
                    # Retry the request
                    response = requests.get(response.request.url, headers=github_headers)
                    if response.status_code != 403:
                        break
                    # Switch to the next token
                    current_token_index = (current_token_index + 1) % len(github_tokens)
                    logging.info(f"Switched to token index {current_token_index}.")
                    # Update the headers with the new token
                    github_headers = get_github_headers()
    return response

# Function to check if a GitHub repository exists
def repo_exists(repo_name):
    url = f"https://api.github.com/repos/{owner}/{repo_name}"
    response = retry_request(requests.get, url, headers=github_headers)
    if response.status_code == 200:
        return True
    elif response.status_code == 404:
        logging.info(f"Repository '{repo_name}' does not exist.")
        return False
    else:
        logging.error(f"Failed to check if repository '{repo_name}' exists: {response.json()}")
        return False

# Function to create a new GitHub repository
def create_github_repo(repo_name):
    url = "https://api.github.com/orgs/KENVUE-DEVEX/repos"
    payload = {
        "name": repo_name,
        "private": True  # Set to True if you want the repo to be private
    }
    response = retry_request(requests.post, url, headers=github_headers, json=payload)
    if response.status_code == 201:
        logging.info(f"Repository '{repo_name}' created successfully.")
    else:
        logging.error(f"Failed to create repository '{repo_name}': {response.json()}")

# Function to upload a file to GitHub
def upload_file_to_github(repo, file_path, file_content):
    url = f"https://api.github.com/repos/{owner}/{repo}/contents/{file_path}"
    payload = {
        "message": f"Upload {file_path}",
        "content": base64.b64encode(file_content).decode('utf-8')
    }
    response = retry_request(requests.put, url, headers=github_headers, json=payload)
    if response.status_code == 201:
        return response.json()
    else:
        logging.error(f"Failed to upload file to GitHub: {response.json()}")
        return None

# Function to parse attachments from the "Attachment" field
def parse_attachments(attachment_field):
    attachments = []
    if pd.notna(attachment_field):
        attachment_entries = attachment_field.split('|||')
        for entry in attachment_entries:
            parts = entry.split(';')
            if len(parts) >= 4:
                timestamp = parts[0].strip()
                user_id = parts[1].strip()
                filename = parts[2].strip()
                url = parts[3].strip()
                attachments.append({
                    'timestamp': timestamp,
                    'user_id': user_id,
                    'filename': filename,
                    'url': url
                })
    return attachments

# Function to process attachments in description and comments
def process_attachments(repo, issue_id, attachment_field):
    attachments = parse_attachments(attachment_field)
    attachment_urls = {}
    for attachment in attachments:
        file_name = attachment['filename']
        file_url = attachment['url']
        
        # Include authentication headers
        headers = {
            "Authorization": f"Basic {encoded_credentials}"
        }
        
        response = requests.get(file_url, headers=headers)
        if response.status_code == 200:
            file_content = response.content
            file_path = f"{issue_id}/{file_name}"
            upload_response = upload_file_to_github(repo, file_path, file_content)
            if upload_response:
                github_file_url = upload_response['content']['html_url']
                attachment_urls[file_name] = github_file_url
        else:
            logging.error(f"Failed to download file from JIRA: {response.json()}")
    
    return json.dumps(attachment_urls)  # Convert dictionary to JSON string

# Function to update issue description
def update_issue_description(repo, issue_number, description):
    url = f"https://api.github.com/repos/{owner}/{repo}/issues/{issue_number}"
    payload = {
        "body": description
    }
    response = retry_request(requests.patch, url, headers=github_headers, json=payload)
    if response.status_code == 200:
        logging.info(f"Issue #{issue_number} description updated successfully.")
    else:
        logging.error(f"Failed to update issue #{issue_number} description: {response.json()}")

# Function to convert text to Markdown
def convert_to_markdown(description, created, creator, parent_issue_id=None, children_issue_ids=None, attachment_urls=None, other_issue_links=None):
    markdown_description = (
        f"**JIRA Description:**\n{description}\n\n"
        f"**Created:** {created}\n"
        f"**Creator:** {creator}\n"
    )
    if pd.notna(parent_issue_id) and isinstance(parent_issue_id, str) and parent_issue_id.strip():
        markdown_description += f"\n**Parent Issue:**\n#{parent_issue_id}\n"
    if pd.notna(children_issue_ids) and isinstance(children_issue_ids, str) and children_issue_ids.strip():
        children_links = "\n".join([f"#{child_id.strip()}" for child_id in children_issue_ids.split(",")])
        markdown_description += f"\n**Children Issues:**\n{children_links}\n"
    if other_issue_links and isinstance(other_issue_links, dict):
        markdown_description += "\n**Other Issue Links:**\n"
        for link_type, issue_ids in other_issue_links.items():
            if pd.notna(issue_ids) and isinstance(issue_ids, list):
                issue_links = ", ".join([f"#{issue_id}" for issue_id in issue_ids])
                markdown_description += f"- **{link_type}:** {issue_links}\n"
    if attachment_urls and isinstance(attachment_urls, dict):
        markdown_description += "\n**Attachments:**\n"
        for file_name, file_url in attachment_urls.items():
            markdown_description += f"- [{file_name}]({file_url})\n"
    return markdown_description

# Make API call to avoid secondary rate limit
def make_random_api_call():
    url = "https://api.github.com/rate_limit"  # A simple endpoint to check rate limits
    response = requests.get(url, headers=github_headers)
    time.sleep(1)  # Throttle by adding a 1-second delay after the random API call
    return response

# Function to process issues
def process_issues(df, repo_name, csv_file):
    for index, row in df.iterrows():
        if pd.notna(row['github_issue_id']):
            logging.info(f"Skipping issue {row['Issue Key']} as it has already been processed.")
            continue

        title = row['Summary']
        description = row['Description']
        created = row['Created']
        creator = row['Creator']
        issue_type = row['Issue Type'] if pd.notna(row['Issue Type']) else ''
        priority = row['Priority'] if pd.notna(row['Priority']) else ''
        status = row['Status'] if pd.notna(row['Status']) else ''

        # Extract original labels from the 'Labels' field
        original_labels = row['Labels'].split(',') if 'Labels' in row and pd.notna(row['Labels']) else []

        # Sanitize and combine all labels
        labels = []
        if issue_type:
            labels.append(sanitize_label_name(issue_type))
        if priority:
            labels.append(sanitize_label_name(priority))
        if status:
            labels.append(sanitize_label_name(status))
        labels += [sanitize_label_name(label) for label in original_labels]

        # Check and create labels if they don't exist
        for label in labels:
            if label and not label_exists(repo_name, label):
                create_label(repo_name, label)

        # Parse comments
        comments = parse_comments(row['Comment']) if 'Comment' in row and pd.notna(row['Comment']) else []
        markdown_comments = convert_comments_to_markdown(comments)

        # Process attachments if not already processed
        if pd.isna(row['github_attachment_urls']):
            issue_id = row['Issue Key']
            attachment_field = row['Attachment'] if 'Attachment' in row else ''
            if attachment_field:
                attachment_urls = process_attachments(repo_name, issue_id, attachment_field)
            else:
                attachment_urls = '[]'  # No attachments, set to empty JSON array
            df.at[index, 'github_attachment_urls'] = attachment_urls  # Store JSON string even if issue creation fails
        else:
            attachment_urls = row['github_attachment_urls']  # Use existing attachment URLs

        # Create the issue in GitHub
        full_description = convert_to_markdown(description, created, creator, attachment_urls=json.loads(attachment_urls))
        issue_number = create_issue(repo_name, title, full_description, labels)

        if issue_number:
            # Add comments to the issue
            for comment in markdown_comments:
                add_comment(repo_name, issue_number, comment)

            # Close the issue if the status indicates it should be closed
            if status in ["Closed", "Completed"]:
                close_issue(repo_name, issue_number)

            # Store the GitHub issue ID and status in the DataFrame
            df.at[index, 'github_issue_id'] = issue_number
            df.at[index, 'github_issue_status'] = "created"
        else:
            # Update the status to indicate failure
            df.at[index, 'github_issue_status'] = "failed to create"

        # Introduce a 20-second delay after each issue is created
        time.sleep(20)

        # Save progress to CSV after processing each issue
        df.to_csv(csv_file, index=False)

# Main script logic
def main():
    parser = argparse.ArgumentParser(description='Process some variables.')
    parser.add_argument('--directory', type=str, required=True, help='Directory to store the CSV file')

    args = parser.parse_args()

    directory = args.directory
    csv_file = os.path.join(directory, 'jira_issues.csv')
    log_file = os.path.join(directory, 'issue_creation.log')

    # Set up logging
    logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    logging.info("Script started.")

    # Check if the directory and CSV file exist
    if not os.path.exists(directory):
        logging.error(f"Directory '{directory}' does not exist.")
        return
    if not os.path.isfile(csv_file):
        logging.error(f"CSV file '{csv_file}' does not exist in the directory '{directory}'.")
        return
    
    # Read the CSV file
    df = pd.read_csv(csv_file)
    print(df.index)
    df.reset_index(inplace=True)
    
    # Strip any leading/trailing spaces from column names
    df.columns = df.columns.str.strip()

    # Print the columns of the DataFrame
    logging.info(f"DataFrame columns after loading CSV: {df.columns.tolist()}")
    logging.info(f"DataFrame head after loading CSV:\n{df.head()}")
    
    # Print the first column header
    first_column_header = df.columns[0]
    print(f"The first column header is: {first_column_header}")
    
    # Print the first value under the 'Issue Key' column
    first_issue_key_value = df['Issue Key'].iloc[0]
    print(f"The first value under 'Issue Key' column is: {first_issue_key_value}")

    # Sort the CSV file based on the "Issue Key" column
    df = df.sort_values(by='Issue Key')

    # Extract the "Project name" from the first non-blank row
    repo_name = sanitize_repo_name(df['Project name'].dropna().iloc[0]) + "-Jira"
    logging.info(f"Creating GitHub repository with name: {repo_name}")

    # Check if the repository exists
    if repo_exists(repo_name):
        logging.info(f"Repository '{repo_name}' already exists. Proceeding with issue creation.")
    else:
        logging.info(f"Repository '{repo_name}' does not exist. Creating new repository.")
        # Create a new GitHub repository with the extracted name
        create_github_repo(repo_name)

    # Add new columns for GitHub issue IDs, statuses, and parent-child relationships if they don't exist
    if 'github_issue_id' not in df.columns:
        df['github_issue_id'] = None
    if 'github_issue_status' not in df.columns:
        df['github_issue_status'] = None
    if 'parent_child_status' not in df.columns:
        df['parent_child_status'] = None
    if 'children_github_issueid' not in df.columns:
        df['children_github_issueid'] = None
    if 'parent_github_issueid' not in df.columns:
        df['parent_github_issueid'] = None
    if 'non_hierarchical_link_github_issueid' not in df.columns:
        df['non_hierarchical_link_github_issueid'] = None
    if 'non_hierarchical_link_status' not in df.columns:
        df['non_hierarchical_link_status'] = None
    if 'github_attachment_urls' not in df.columns:  # New column for GitHub attachment URLs
        df['github_attachment_urls'] = None

    # Process issues until all are created
    while df['github_issue_id'].isna().any():
        process_issues(df, repo_name, csv_file)
        # Reload the CSV file to get the latest updates
        df = pd.read_csv(csv_file)

    # Create an index for fast lookups
    df.set_index('Issue Key', inplace=True)

    # Handle parent-child relationships in a separate loop
    for index, row in df.iterrows():
        if row['parent_child_status'] == "updated" or row['parent_child_status'] == "no parent":
            logging.info(f"Skipping parent-child update for issue {row['Issue Key']} as it has already been processed.")
            continue

        parent_key = row['Parent']
        epic_link = row['Epic Link']

        # Determine the parent key to use
        if pd.notna(parent_key):
            parent_issue_key = parent_key
        elif pd.notna(epic_link):
            parent_issue_key = epic_link
        else:
            parent_issue_key = None

        if parent_issue_key:
            # Use the index to quickly find the parent GitHub issue ID
            parent_github_issue_id = df.at[parent_issue_key, 'github_issue_id'] if parent_issue_key in df.index else None

            if parent_github_issue_id:
                df.at[index, 'parent_github_issueid'] = parent_github_issue_id

                # Add this issue to the parent's children list
                if pd.isna(df.at[parent_issue_key, 'children_github_issueid']):
                    df.at[parent_issue_key, 'children_github_issueid'] = str(df.at[index, 'github_issue_id'])
                else:
                    df.at[parent_issue_key, 'children_github_issueid'] += f", {df.at[index, 'github_issue_id']}"

                df.at[index, 'parent_child_status'] = "updated"
            else:
                df.at[index, 'parent_child_status'] = "parent not found"
        else:
            df.at[index, 'parent_child_status'] = "no parent"

    # Handle non-hierarchical links in a separate loop
    for index, row in df.iterrows():
        if row['non_hierarchical_link_status'] == "updated":
            logging.info(f"Skipping non-hierarchical link update for issue {row['Issue Key']} as it has already been processed.")
            continue

        non_hierarchical_links = row['Non-Hierarchical Link'] if 'Non-Hierarchical Link' in row else ''
        if pd.notna(non_hierarchical_links):
            link_types = non_hierarchical_links.split('|||')
            other_issue_links = {}
            for link_type in link_types:
                parts = link_type.split(';')
                if len(parts) >= 2:
                    link_type_name = parts[0].strip()
                    linked_issue_keys = parts[1].strip().split(',')
                    linked_github_issue_ids = [df.at[linked_issue_key, 'github_issue_id'] for linked_issue_key in linked_issue_keys if linked_issue_key in df.index]
                    other_issue_links[link_type_name] = linked_github_issue_ids

            if other_issue_links:
                df.at[index, 'non_hierarchical_link_status'] = "updated"
                df.at[index, 'other_issue_links'] = other_issue_links  # Store the processed links
            else:
                df.at[index, 'non_hierarchical_link_status'] = "no links"

    # Updating issue description with parent, child, and other issue links
    for index, row in df.iterrows():
        if pd.notna(row['github_issue_id']):
            # Extract the necessary fields from the row
            description = row['Description']
            created = row['Created']
            creator = row['Creator']

            # Convert JSON string back to dictionary
            attachment_urls = json.loads(row['github_attachment_urls']) if 'github_attachment_urls' in row and pd.notna(row['github_attachment_urls']) else {}

            # Generate the full description with updated information
            full_description = convert_to_markdown(
                description, created, creator,
                parent_issue_id=row['parent_github_issueid'] if pd.notna(row['parent_github_issueid']) else None,
                children_issue_ids=row['children_github_issueid'] if pd.notna(row['children_github_issueid']) else None,
                attachment_urls=attachment_urls,
                other_issue_links=row['other_issue_links'] if 'other_issue_links' in row and pd.notna(row['other_issue_links']) else {}
            )

            # Update the issue description in GitHub
            update_issue_description(repo_name, row['github_issue_id'], full_description)

            # Introduce a 25-second delay after each update description API call
            time.sleep(25)

    # Save the final DataFrame to CSV
    df.to_csv(csv_file, index=True)

    logging.info("Script completed.")

if __name__ == "__main__":
    main()
